> [!NOTE] 
>- 論文標題：Autoregressive Models in Vision: A Survey
>- 論文連結: https://arxiv.org/abs/2411.05902
>- 專案地址： https://github.com/ChaofanTao/Autoregressive-Models-in-Vision-Survey?tab=readme-ov-file

**本文是一篇關於自迴歸模型在視覺領域發展的綜述論文，由港大、清華、普林斯頓、杜克、俄亥俄州立、UNC、蘋果、位元組跳動、香港理工大學等多所高校及研究機構的夥伴聯合釋出。**

隨著計算機視覺領域的不斷髮展，自迴歸模型作為一種強大的生成模型，在影象生成、視訊生成、3D 生成和多模態生成等任務中展現出了巨大的潛力。然而，由於該領域的快速發展，及時、全面地瞭解自迴歸模型的研究現狀和進展變得至關重要。本文旨在對視覺領域中的自迴歸模型進行全面綜述，為研究人員提供一個清晰的參考框架。

![[主題記錄/AI/語言模型/attachments/19d5ee0d07091acbf4bbb7c2816965c8_MD5.png]]

研究的主要亮點如下：

**最新最全的文獻綜述：**本文對視覺領域中的自迴歸模型進行了全面的文獻綜述，涵蓋了約 250 篇相關參考文獻，包括一些新興領域的相關文獻，比如 3D 醫療、具身智慧等。通過對這些文獻的整理和分析，本文能夠為讀者提供一個系統的瞭解自迴歸模型在視覺領域的發展歷程和研究現狀的有效幫助。

![[主題記錄/AI/語言模型/attachments/e2adfa5404fa4b63b4336be04ef1321d_MD5.png]]

**基於序列表徵的分類：**本文根據序列表示策略對自迴歸模型進行了分類，包括基於 pixel、基於 token 和基於 scale 的視覺自迴歸模型。同時，本文還對不同型別的自迴歸模型在影象生成、視訊生成、3D 生成和多模態生成等任務中的效能進行了比較和分析。通過這些分類和比較，本文能夠幫助讀者更好地理解不同型別的自迴歸模型的特點和優勢，為選擇合適的模型提供參考。

![[主題記錄/AI/語言模型/attachments/b57afde97ffb9d09a2b983af5025f611_MD5.png]]

左邊圖展示的是 3 種主流的用於自迴歸視覺模型的表徵方法。右邊圖展示的是自迴歸視覺模型的主要組成：序列表徵方法和自迴歸序列建模方式。

**各種領域的應用總結：**本文詳細介紹了自迴歸模型在影象生成、視訊生成、3D 生成和多模態生成等任務中的應用。通過對這些應用的總結和分析，本文能夠為讀者展示自迴歸模型在不同領域的應用潛力和實際效果，為進一步推動自迴歸模型的應用提供參考。下面是本文的文獻分類框架圖：


![[outline_new.png]]

**挑戰與展望：**本文討論了自迴歸模型在視覺領域面臨的挑戰，如計算複雜度、模式崩潰等，並提出了一些潛在的研究方向。通過對這些挑戰和展望的討論，本文能夠為讀者提供一個思考和探索的方向，促進自迴歸模型在視覺領域的進一步發展。

**2. 視覺自迴歸模型**

**基礎知識**

視覺自迴歸模型有兩個核心的組成部分：序列表示和自迴歸序列建模方法。首先，讓我們來了解這兩個關鍵方面：

序列表示：將視覺資料轉化為離散元素序列，如畫素、視覺詞元等。這種表示方法類似於自然語言處理（NLP）中的文字生成中把詞分成詞元進行後續處理，為自迴歸模型在計算機視覺領域的應用奠定了基礎。舉例來說，對於影象資料，可以將其劃分爲畫素序列或者影象塊序列，每個畫素或影象塊作為序列中的一個元素。這樣，就可以利用自迴歸模型依次預測每個元素，從而實現影象的生成或重建。

自迴歸序列建模：基於先前生成的元素，通過條件概率依次預測每個元素。具體來說，對於一個序列中的第 t 個元素，自迴歸模型會根據前面 t-1 個元素的資訊來預測第 t 個元素的概率分佈。訓練目標是最小化負對數似然損失。通過不斷調整模型引數，使得模型預測的概率分佈儘可能接近真實資料的分佈，從而提高模型的效能。

**2.1 通用框架分類**

瞭解了自迴歸模型的基礎之後，我們接下來看看不同的通用框架分類。下面我們分別介紹基於畫素、基於視覺詞元和基於尺度的模型。

**2.1.1 基於畫素（pixel）的模型：**這類模型直接在畫素級別表示視覺資料，如 PixelRNN 和 PixelCNN 等。PixelRNN 通過遞迴神經網路（RNN）捕捉畫素間的依賴關係，從影象的左上角開始，依次預測每個畫素的值。PixelCNN 則使用摺積神經網路（CNN）來實現畫素級別的自迴歸建模，通過對影象進行卷積操作來獲取畫素間的區域性依賴關係。

但是這類模型在高解析度影象生成時面臨計算成本高和資訊冗餘的挑戰。由於需要對每個畫素進行預測，隨著影象解析度的提高，計算量會呈指數增長。同時，畫素之間的相關性可能導致資訊冗餘，影響模型的效率和效能。

**2.1.2 基於視覺詞元（token）的模型：**將影象壓縮為離散視覺詞元序列，如 VQ-VAE 及其變體。

這類模型先使用編碼器將影象對映到潛在空間並量化為離散程式碼，再用解碼器重建影象。在此基礎上，採用強大的自迴歸模型預測下一個離散視覺詞元。例如，VQ-VAE 通過向量量化將影象編碼為離散的視覺詞元序列，然後使用自迴歸模型對視覺詞元序列進行建模，實現影象的生成和重建。但是這類模型存在碼本利用率低和取樣速度慢的問題。碼本中的視覺詞元可能沒有被充分利用，導致生成的影象質量受限。同時，由於需要依次預測每個視覺詞元，取樣速度相對較慢。

**2.1.3 基於尺度（scale）的模型：**以不同尺度的視覺詞元圖作為自迴歸單元，如 VAR。通過多尺度量化自動編碼器將影象離散化為視覺詞元學習不同解析度的資訊，生成過程從粗到細逐步進行。例如，VAR 首先在低解析度下生成粗糙的視覺詞元圖，然後逐步細化到高解析度，從而提高生成影象的質量和效率。相比基於視覺詞元的模型，它能更好地保留空間區域性性，提高視覺詞元生成效率。通過多尺度的建模方式，可以更好地捕捉影象的區域性結構和細節資訊。

不同的通用框架分類各有特點，而自迴歸模型與其他生成模型也有著緊密的關係。接下來，我們探討自迴歸模型與其他生成模型的關係。

**2.3 與其他生成模型的關係**

自迴歸模型與變分自編碼器（VAEs）、生成對抗網路（GANs）、歸一化流、擴散模型和掩碼自編碼器（MAEs）等生成模型在不同方面有著聯絡和區別。

**變分自編碼器（VAEs）：**VAEs 學習將資料對映到低維潛在空間並重建，而自迴歸模型直接捕捉資料分佈。兩者結合的方法如 VQ-VAE，能有效利用兩者優勢進行影象合成。VQ-VAE 首先通過編碼器將影象對映到潛在空間，然後使用向量量化將潛在空間離散化為視覺詞元序列，最後使用自迴歸模型對視覺詞元序列進行建模，實現影象的生成和重建。

**生成對抗網路（GANs）：**GANs 生成速度快，但訓練不穩定且可能出現模式崩潰。自迴歸模型採用似然訓練，過程穩定，雖取樣速度慢，但模型效能隨資料和模型規模提升。在影象生成任務中，GANs 可以快速生成逼真的影象，但可能會出現模式崩潰的問題，即生成的影象缺乏多樣性。自迴歸模型則可以通過似然訓練保證生成的影象具有較高的質量和多樣性。

**歸一化流 （Normalizing Flows）：**通過一系列可逆變換將簡單分佈對映到複雜資料分佈，與自迴歸模型都可通過最大似然估計直接優化。但歸一化流需保證可逆性，自迴歸模型則通過離散化資料和順序預測更具靈活性。歸一化流需要設計可逆的變換函式，這在實際應用中可能會比較困難。而自迴歸模型可以通過離散化資料和順序預測的方式，更加靈活地捕捉資料的分佈特徵。

**擴散模型 （Diffusion Models）：**與自迴歸模型類似，兩類模型都能生成高質量樣本，但是兩者在生成正規化上有根本區別。當前自迴歸模型已經逐漸在效能上追趕上擴散模型，且展現了很好的scaling到更大模型的潛力。近期研究嘗試結合兩者的優勢，進一步提高生成模型的效能。

**掩碼自編碼器（MAEs）：**MAEs 通過隨機掩碼輸入資料並重建來學習資料表示，與自迴歸模型有相似之處，但訓練方式和注意力機制不同。例如，MAEs 在訓練時隨機掩碼一部分輸入資料，然後通過重建被掩碼的部分來學習資料的表示。自迴歸模型則是通過順序預測的方式來學習資料的分佈。兩者在訓練方式和注意力機制上存在差異。

**3．視覺自迴歸模型的應用**

自迴歸模型在影象生成、視訊生成、3D 生成和多模態生成等任務中都有著廣泛的應用。結合經典的和最新的相關工作，我們做出以下的分類，感興趣的讀者可以在論文中閱讀每個子類的詳情。

**3.1 影象生成**

- 無條件影象生成：畫素級生成逐個畫素構建影象，如 PixelRNN 和 PixelCNN 等。視覺詞元級生成將影象視為視覺詞元序列，如 VQ-VAE 及其改進方法。尺度級生成從低到高解析度逐步生成影象，如 VAR。
- 文字到影象合成：根據文字條件生成影象，如 DALL・E、CogView 等。近期研究還探索了與擴散模型、大語言模型的結合，以及向新任務的擴充套件。
- 影象條件合成：包括影象修復、多檢視生成和視覺上下文學習等，如 QueryOTR 用於影象外繪，MIS 用於多檢視生成，MAE-VQGAN 和 VICL 用於視覺上下文學習。
- 影象編輯：分為文字驅動和影象驅動的影象編輯。文字驅動如 VQGAN-CLIP 和 Make-A-Scene，可根據文字輸入修改影象。影象驅動如 ControlAR、ControlVAR 等，通過控制機制實現更精確的影象編輯。

**3.2 視訊生成**

- 無條件視訊生成：從無到有建立視訊序列，如 Video Pixel Networks、MoCoGAN 等。近期方法如 LVT、VideoGPT 等結合 VQ-VAE 和 Transformer 提高了生成質量。
- 條件視訊生成：根據特定輸入生成視訊，包括文字到視訊合成、視覺條件視訊生成和多模態條件視訊生成。如 IRC-GAN、CogVideo 等用於文字到視訊合成，Convolutional LSTM Network、PredRNN 等用於視覺條件視訊生成，MAGE 用於多模態條件視訊生成。
- 具身智慧：視訊生成在具身智慧中用於訓練和增強智慧體，如學習動作條件視訊預測模型、構建通用世界模型等。

**3.3 3D 生成**

在運動生成、點雲生成、場景生成和 3D 醫學生成等方面取得進展。如 T2M-GPT 用於運動生成，CanonicalVAE 用於點雲生成，Make-A-Scene 用於場景生成，SynthAnatomy 和 BrainSynth 用於 3D 醫學生成。

**3.4 多模態：**

- 多模態理解框架：通過離散影象視覺詞元掩碼影象建模方法學習視覺表示，如 BEiT 及其變體。
- 統一多模態理解和生成框架：將視覺和文字輸出生成相結合，如 OFA、CogView 等早期模型，以及 NEXTGPT、SEED 等近期模型。最近還出現了原生多模態自迴歸模型，如 Chameleon 和 Transfusion。

**3. 評估指標**

評估視覺自迴歸模型的效能需要綜合考慮多個方面的指標。我們從視覺分詞器重建和模型生成的角度分別進行度量：

**視覺分詞器重建評估：**主要關注重建保真度，常用指標包括 PSNR、SSIM、LPIPS 和 rFID 等。例如，PSNR（峰值訊雜比）用於衡量重建影象與原始影象之間的畫素差異，SSIM（結構相似性指數）則考慮了影象的結構資訊和亮度、對比度等因素。

**視覺自迴歸生成評估：**包括視覺質量（如負對數似然、Inception Score、Fréchet Inception Distance 等）; 多樣性（如 Precision 和 Recall、MODE Score 等）; 語義一致性（如 CLIP Score、R-precision 等）; 時間一致性（如 Warping Errors、CLIPSIM-Temp 等）; 以人為中心的評估（如人類偏好分數、Quality ELO Score 等）。

另外，我們在論文中總結了自迴歸模型、Diffusion、GAN、MAE 等生成方法在四個常用的影象生成基準上（例如 MSCOCO）的表現，揭示了當前自迴歸視覺生成方法與 SOTA 方法的差距。

![[主題記錄/AI/語言模型/attachments/15e708cc17644210bab3dc82c93d3398_MD5.png]]

![[主題記錄/AI/語言模型/attachments/10887e1b3ba360ae6a44e2a1cef37402_MD5.png]]



![[主題記錄/AI/語言模型/attachments/c23e57096323e8b265ac1212aeddbebc_MD5.png]]

**5. 挑戰與未來工作**

自迴歸模型在計算機視覺領域雖然取得了一定的成果，但也面臨著一些挑戰:

**5.1 視覺分詞器設計：**設計能有效壓縮影象或視訊的視覺分詞器是關鍵挑戰，如 VQGAN 及其改進方法，以及利用層次多尺度特性提高壓縮效果。例如，可以通過改進向量量化演算法、引入注意力機制等方式，提高視覺分詞器的效能和壓縮效果。

**5.2 離散與連續表徵的選擇：**自迴歸模型傳統上採用離散表示，但連續表示在簡化視覺數據壓縮器訓練方面有優勢，同時也帶來新挑戰，如損失函式設計和多模態適應性。例如，可以探索連續表示下的自迴歸模型，設計合適的損失函式，提高模型在多模態資料上的適應性。

**5.3 自迴歸模型架構中的歸納偏差：**探索適合視覺訊號的歸納偏差架構，如 VAR 利用層次多尺度視覺詞元化，以及雙向注意力的優勢。例如，可以研究不同的歸納偏差架構對自迴歸模型效能的影響，尋找最適合視覺訊號的架構。

**5.4 下游任務：**當前視覺自迴歸模型在下游任務上的研究相對滯後，未來需開發能適應多種下游任務的統一自迴歸模型。例如，可以將自迴歸模型應用於目標檢測、語義分割等下游任務，探索如何提高模型在這些任務上的效能。

**6. 總結**

本文對計算機視覺中的自迴歸模型進行了全面綜述，介紹了自迴歸模型的基礎、通用框架分類、與其他生成模型的關係、應用領域、評估指標以及面臨的挑戰和未來工作。自迴歸模型在計算機視覺領域具有廣闊的應用前景，但仍需進一步研究解決現有問題，以推動其發展和應用。

文章來源：[機器之心](https://www.jiqizhixin.com/articles/2024-12-01-3)

COPYRIGHT ©2020, BANGQU.COM | [Coupons](http://bangqu.com/coupons)