Coursera 課程筆記

# Chap 1 

## Q & A 練習：

*Bert 有那些類？*

- **Masked Language Model (MLM):** This model is trained to predict the masked words in a sentence.
- **Next Sentence Prediction (NSP):** This model is trained to predict whether two sentences are consecutive or not.
- **Whole Word Masking (WWM):** This model is trained to predict the masked words in a sentence, but instead of masking individual words, it masks entire words.

*Transformer model 有那三種 embeddings？*
* token 
* segment
* position

