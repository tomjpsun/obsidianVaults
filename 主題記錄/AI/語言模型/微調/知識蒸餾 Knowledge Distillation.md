> 資訊揭露：此文章有經過 AI 輔助撰寫後，再進行內文修改，如有錯誤再請告知，感謝大家！

![[主題記錄/AI/語言模型/微調/attachments/5b97cb4566b893c1126b92b8514243aa_MD5.png]]

#知識蒸餾 #KnowledgeDistillation

# 一、知識蒸餾概述

知識蒸餾（Knowledge Distillation）是一種深度學習中的模型壓縮技術，其核心思想在於透過「教師 — 學生」的學習架構，將大型且表現卓越的教師模型中蘊含的知識轉移至體積更小、計算成本更低的學生模型中。由 Geoffrey Hinton 等人於 2015 年提出後，該技術迅速應用於自然語言處理、計算機視覺等多個領域，並成為在資源受限環境中部署高效能模型的重要手段。

![[主題記錄/AI/語言模型/微調/attachments/20c4ebb7a3b0ca1e6760e1a54daa8be2_MD5.jpg]]

老師教導學生 — 示意圖

在知識蒸餾中，教師模型通常經過長時間、大規模資料集訓練，獲得優異的泛化能力，但由於參數規模龐大、運算量高，其在邊緣設備或 API 應用中難以直接部署；相對地，學生模型則在結構上更簡潔、計算開銷更低，經由學習教師模型及其他中間知識，可以在保持高準確率的同時達到輕量化與高效運算的目的。

# 二、核心原理與基本流程

## 1. 教師與學生模型架構

- **教師模型（Teacher Model）**  
    一般為經過充分預訓練的大型神經網路，能夠從數據中捕捉到豐富且細緻的知識。教師模型的輸出（通常為未經硬性標籤約束的邏輯或中間層特徵）包含了多層次、隱含的訊息，是知識轉移的主要來源。
- **學生模型（Student Model）**  
    相對於教師模型，學生模型結構簡單、參數較少。學生模型透過模仿教師模型的輸出，學習其對輸入資料所做出的預測分布，以期在保持精度的同時大幅降低計算與部署成本。

## 2. 軟/硬標籤與溫度參數

傳統模型訓練使用「硬標籤（可以看作是你有沒有及格達標）」，而知識蒸餾則利用教師模型輸出的「軟標籤（可以視作是考試成績）」，即各類別預測機率分布。

透過引入溫度（Temperature）參數調整 softmax 輸出，可以使得原本極端的機率分布變得平滑，進一步了解教師模型對於各類別之間相似度的微妙判斷。溫度 T 在訓練階段通常設定較大，為了讓知識轉移上能夠儘可能轉移到學生模型，等待學生模型訓練完成後，再調回正常較小的溫度，以用於推理。

![[主題記錄/AI/語言模型/微調/attachments/f6a778078ef860deae2a50d572ace697_MD5.png]]

一個示範範例，了解 Softmax Function 和 Temperature 的關係

## 3. 損失函數設計

知識蒸餾的訓練損失通常結合兩部分：

- **蒸餾損失（Distillation Loss）**：衡量學生模型與教師模型之間軟標籤的相似度，常採用 KL 散度或 Cross entropy 計算。此部分主要負責讓學生模仿教師模型的預測分布。

> 備註一：KL 散度是一種**衡量兩個機率分布之間差異**的方式，常用於量化模型的預測分布與目標分布（如教師模型與學生模型輸出的差異）。
> 
> 備註二：Cross entropy 是一種衡量**一個機率分布如何描述另一個機率分布**的方式，特別適用於分類問題中模型輸出與目標標籤的比較。

- **學生損失（Student Loss）**：對學生模型與真實標籤之間的標準 Cross entropy loss，確保模型仍保有與原始任務相符的判斷能力。

這兩部分透過一個權重參數（例如超參數 α）進行平衡，以達到既能模仿教師知識又不偏離真實標籤的最佳效果。

## 4. 擴展知識轉移方式

除了直接利用最終輸出（response-based knowledge），知識蒸餾亦可進一步挖掘教師模型中間層特徵（feature-based knowledge）或樣本間關聯（relation-based knowledge），這些方法能夠使學生模型更全面地捕捉教師模型的內部表示與判斷邏輯，提高其泛化能力。

# 三、技術優勢

- **計算效率顯著提升**：透過知識蒸餾，學生模型在運算量與記憶體佔用上均顯著降低，適合部署於資源有限的邊緣設備或 real-time API 應用中。
- **模型部署靈活性增強**：小型化的學生模型在硬體成本、能源消耗及運行速度上均優於大型教師模型，滿足商業化大規模應用的需求。
- **知識泛化效果良好**：在不顯著損失精度的前提下，學生模型學習到教師模型中隱含的細微訊息，有助於在多種任務上表現出穩定且接近大型模型的效果。

# 四、DeepSeek 與知識蒸餾的實踐

近期 DeepSeek 針對知識蒸餾技術的應用引起業界高度關注。根據公開資料，DeepSeek 不僅在其部分模型（如 DeepSeek-R1-Distill）中應用了知識蒸餾技術，還利用此方法從其他預訓練的開放權重模型（例如 LLaMA、Qwen 等）中萃取知識，再透過微調與合成數據使得學生模型在性能上接近甚至超越原始大型模型。

[DeepSeek-R1 - a deepseek-ai Collection](https://huggingface.co/collections/deepseek-ai/deepseek-r1-678e1e131c0169c0bc89728d?source=post_page-----cea0e5d6d842--------------------------------)

此外，來自 AI 領域的專家也提及，這種利用蒸餾技術壓縮模型知識的方法，不僅能大幅降低訓練與部署成本，同時也能促進技術普及，加速整個產業的競爭與創新。

DeepSeek 的實踐表明，藉由知識蒸餾技術，小型模型可藉由學習教師模型輸出的細節資訊，從而在邏輯推理、數學運算、程式生成等多項任務上達到與大型模型相仿甚至超越的效果。這一成果在市場上引發熱議，並對全球 AI 產業的技術競爭格局產生深遠影響。

![[主題記錄/AI/語言模型/微調/attachments/c1b235ae35e19e0700375d88273a643c_MD5.png]]

LLM Evaluation (Source: [Internet](https://arxiv.org/html/2501.12948v1))

# 五、觀點

從企業戰略角度看，知識蒸餾技術正日益成為 AI 模型輕量化與快速部署的關鍵技術。隨著邊緣運算、物聯網以及即時互動應用需求的持續上升，利用 知識蒸餾技術構建高效、低成本的小型模型，將是企業進行數位轉型和技術創新不可或缺的選擇。此外，透過開放源碼和跨模型知識融合，企業還可以加速技術迭代，實現從多模型集成到單一高效模型的跨越，從而在激烈的全球市場競爭中獲得先機。

DeepSeek 的實例更進一步驗證了這一趨勢：在面臨資源限制下，透過知識蒸餾技術有效提升模型效能與成本效益。未來，隨著技術的不斷演進與應用場景的拓展，知識蒸餾有望在更多領域中發揮關鍵作用，並推動全球 AI 技術向更普惠、更高效的方向發展。

[Source](https://medium.com/@simon3458/intro-knowledge-distillation-cea0e5d6d842)
